{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a838620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\n",
      "-rw-r--r--   2 hdfs supergroup          0 2021-01-28 11:23 /public/hr_db/employees/_SUCCESS\n",
      "-rw-r--r--   2 hdfs supergroup       2123 2021-01-28 09:29 /public/hr_db/employees/part-m-00000\n",
      "-rw-r--r--   2 hdfs supergroup       2159 2021-01-28 10:54 /public/hr_db/employees/part-m-00001\n",
      "-rw-r--r--   2 hdfs supergroup       2145 2021-01-28 08:16 /public/hr_db/employees/part-m-00002\n",
      "-rw-r--r--   2 hdfs supergroup       2121 2021-01-28 09:29 /public/hr_db/employees/part-m-00003\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /public/hr_db/employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0854421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\tNanette\tCambrault\tNCAMBRAU\t011.44.1344.987668\t1998-12-09\tSA_REP\t7500.00\t0.20\t145\t80\n",
      "155\tOliver\tTuvault\tOTUVAULT\t011.44.1344.486508\t1999-11-23\tSA_REP\t7000.00\t0.15\t145\t80\n",
      "156\tJanette\tKing\tJKING\t011.44.1345.429268\t1996-01-30\tSA_REP\t10000.00\t0.35\t146\t80\n",
      "157\tPatrick\tSully\tPSULLY\t011.44.1345.929268\t1996-03-04\tSA_REP\t9500.00\t0.35\t146\t80\n",
      "158\tAllan\tMcEwen\tAMCEWEN\t011.44.1345.829268\t1996-08-01\tSA_REP\t9000.00\t0.35\t146\t80\n",
      "159\tLindsey\tSmith\tLSMITH\t011.44.1345.729268\t1997-03-10\tSA_REP\t8000.00\t0.30\t146\t80\n",
      "160\tLouise\tDoran\tLDORAN\t011.44.1345.629268\t1997-12-15\tSA_REP\t7500.00\t0.30\t146\t80\n",
      "161\tSarath\tSewall\tSSEWALL\t011.44.1345.529268\t1998-11-03\tSA_REP\t7000.00\t0.25\t146\t80\n",
      "162\tClara\tVishney\tCVISHNEY\t011.44.1346.129268\t1997-11-11\tSA_REP\t10500.00\t0.25\t147\t80\n",
      "163\tDanielle\tGreene\tDGREENE\t011.44.1346.229268\t1999-03-19\tSA_REP\t9500.00\t0.15\t147\t80\n",
      "164\tMattea\tMarvins\tMMARVINS\t011.44.1346.329268\t2000-01-24\tSA_REP\t7200.00\t0.10\t147\t80\n",
      "165\tDavid\tLee\tDLEE\t011.44.1346.529268\t2000-02-23\tSA_REP\t6800.00\t0.10\t147\t80\n",
      "166\tSundar\tAnde\tSANDE\t011.44."
     ]
    }
   ],
   "source": [
    "!hadoop fs -head /public/hr_db/employees/part-m-00002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5645d677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass as gp\n",
    "from pyspark.sql import SparkSession, functions as F, types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "504ba7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv005077'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user = gp.getuser()\n",
    "user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9f05ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(f'{user}-dataframe-writer-example') \\\n",
    "    .master('yarn') \\\n",
    "    .config('spark.sql.warehouse.dir', f'/user/{user}/warehouse') \\\n",
    "    .config('saprk.sql.catalogImplementation', 'hive') \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ad93df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://g02.itversity.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>itv005077-dataframe-writer-example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f84c3cf2a20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96439d00",
   "metadata": {},
   "source": [
    "# Read from parquet file directly without using reader API's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcc7dc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------+---------------+\n",
      "|customer_id|          order_date|order_id|   order_status|\n",
      "+-----------+--------------------+--------+---------------+\n",
      "|      11599|2013-07-25 00:00:...|       1|         CLOSED|\n",
      "|        256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|\n",
      "|      12111|2013-07-25 00:00:...|       3|       COMPLETE|\n",
      "|       8827|2013-07-25 00:00:...|       4|         CLOSED|\n",
      "|      11318|2013-07-25 00:00:...|       5|       COMPLETE|\n",
      "|       7130|2013-07-25 00:00:...|       6|       COMPLETE|\n",
      "|       4530|2013-07-25 00:00:...|       7|       COMPLETE|\n",
      "|       2911|2013-07-25 00:00:...|       8|     PROCESSING|\n",
      "|       5657|2013-07-25 00:00:...|       9|PENDING_PAYMENT|\n",
      "|       5648|2013-07-25 00:00:...|      10|PENDING_PAYMENT|\n",
      "|        918|2013-07-25 00:00:...|      11| PAYMENT_REVIEW|\n",
      "|       1837|2013-07-25 00:00:...|      12|         CLOSED|\n",
      "|       9149|2013-07-25 00:00:...|      13|PENDING_PAYMENT|\n",
      "|       9842|2013-07-25 00:00:...|      14|     PROCESSING|\n",
      "|       2568|2013-07-25 00:00:...|      15|       COMPLETE|\n",
      "|       7276|2013-07-25 00:00:...|      16|PENDING_PAYMENT|\n",
      "|       2667|2013-07-25 00:00:...|      17|       COMPLETE|\n",
      "|       1205|2013-07-25 00:00:...|      18|         CLOSED|\n",
      "|       9488|2013-07-25 00:00:...|      19|PENDING_PAYMENT|\n",
      "|       9198|2013-07-25 00:00:...|      20|     PROCESSING|\n",
      "+-----------+--------------------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM parquet.`/public/trendytech/datasets/ordersparquet`\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9389517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = T.StructType([\n",
    "    T.StructField('employee_id', T.IntegerType()),\n",
    "    T.StructField('first_name', T.StringType()),\n",
    "    T.StructField('last_name', T.StringType()),\n",
    "    T.StructField('email', T.StringType()),\n",
    "    T.StructField('phone_number', T.StringType()),\n",
    "    T.StructField('hire_date', T.DateType()),\n",
    "    T.StructField('job_id', T.StringType()),\n",
    "    T.StructField('salary', T.FloatType()),\n",
    "    T.StructField('commission_pct', T.FloatType()),\n",
    "    T.StructField('manager_id', T.IntegerType()),\n",
    "    T.StructField('department_id', T.IntegerType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "336cd597",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emp = spark.read \\\n",
    "    .format('csv') \\\n",
    "    .option('delimiter', '\\t') \\\n",
    "    .schema(schema) \\\n",
    "    .load('/public/hr_db/employees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1e6cfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+--------+------------------+----------+--------+-------+--------------+----------+-------------+\n",
      "|employee_id|first_name| last_name|   email|      phone_number| hire_date|  job_id| salary|commission_pct|manager_id|department_id|\n",
      "+-----------+----------+----------+--------+------------------+----------+--------+-------+--------------+----------+-------------+\n",
      "|        127|     James|    Landry| JLANDRY|      650.124.1334|1999-01-14|ST_CLERK| 2400.0|          null|       120|           50|\n",
      "|        128|    Steven|    Markle| SMARKLE|      650.124.1434|2000-03-08|ST_CLERK| 2200.0|          null|       120|           50|\n",
      "|        129|     Laura|    Bissot| LBISSOT|      650.124.5234|1997-08-20|ST_CLERK| 3300.0|          null|       121|           50|\n",
      "|        130|     Mozhe|  Atkinson|MATKINSO|      650.124.6234|1997-10-30|ST_CLERK| 2800.0|          null|       121|           50|\n",
      "|        131|     James|    Marlow| JAMRLOW|      650.124.7234|1997-02-16|ST_CLERK| 2500.0|          null|       121|           50|\n",
      "|        132|        TJ|     Olson| TJOLSON|      650.124.8234|1999-04-10|ST_CLERK| 2100.0|          null|       121|           50|\n",
      "|        133|     Jason|    Mallin| JMALLIN|      650.127.1934|1996-06-14|ST_CLERK| 3300.0|          null|       122|           50|\n",
      "|        134|   Michael|    Rogers| MROGERS|      650.127.1834|1998-08-26|ST_CLERK| 2900.0|          null|       122|           50|\n",
      "|        135|        Ki|       Gee|    KGEE|      650.127.1734|1999-12-12|ST_CLERK| 2400.0|          null|       122|           50|\n",
      "|        136|     Hazel|Philtanker|HPHILTAN|      650.127.1634|2000-02-06|ST_CLERK| 2200.0|          null|       122|           50|\n",
      "|        137|    Renske|    Ladwig| RLADWIG|      650.121.1234|1995-07-14|ST_CLERK| 3600.0|          null|       123|           50|\n",
      "|        138|   Stephen|    Stiles| SSTILES|      650.121.2034|1997-10-26|ST_CLERK| 3200.0|          null|       123|           50|\n",
      "|        139|      John|       Seo|    JSEO|      650.121.2019|1998-02-12|ST_CLERK| 2700.0|          null|       123|           50|\n",
      "|        140|    Joshua|     Patel|  JPATEL|      650.121.1834|1998-04-06|ST_CLERK| 2500.0|          null|       123|           50|\n",
      "|        141|    Trenna|      Rajs|   TRAJS|      650.121.8009|1995-10-17|ST_CLERK| 3500.0|          null|       124|           50|\n",
      "|        142|    Curtis|    Davies| CDAVIES|      650.121.2994|1997-01-29|ST_CLERK| 3100.0|          null|       124|           50|\n",
      "|        143|   Randall|     Matos|  RMATOS|      650.121.2874|1998-03-15|ST_CLERK| 2600.0|          null|       124|           50|\n",
      "|        144|     Peter|    Vargas| PVARGAS|      650.121.2004|1998-07-09|ST_CLERK| 2500.0|          null|       124|           50|\n",
      "|        145|      John|   Russell| JRUSSEL|011.44.1344.429268|1996-10-01|  SA_MAN|14000.0|           0.4|       100|           80|\n",
      "|        146|     Karen|  Partners|KPARTNER|011.44.1344.467268|1997-01-05|  SA_MAN|13500.0|           0.3|       100|           80|\n",
      "+-----------+----------+----------+--------+------------------+----------+--------+-------+--------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb9d86c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emp.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56835b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.defaultMinPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b4b63d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emp.write \\\n",
    ".format('csv') \\\n",
    ".option('header', True) \\\n",
    ".mode('overwrite') \\\n",
    ".save(f'/user/{user}/spark_write/non-part/default/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1d4f7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   3 itv005077 supergroup          0 2023-07-02 15:49 spark_write/non-part/default/_SUCCESS\n",
      "-rw-r--r--   3 itv005077 supergroup      4.2 K 2023-07-02 15:49 spark_write/non-part/default/part-00000-3c9f5af5-def1-4997-b1e6-fbac2b33c3db-c000.csv\n",
      "-rw-r--r--   3 itv005077 supergroup      4.1 K 2023-07-02 15:49 spark_write/non-part/default/part-00001-3c9f5af5-def1-4997-b1e6-fbac2b33c3db-c000.csv\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls -h spark_write/non-part/default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "913b7f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employee_id,first_name,last_name,email,phone_number,hire_date,job_id,salary,commission_pct,manager_id,department_id\n",
      "100,Steven,King,SKING,515.123.4567,1987-06-17,AD_PRES,24000.0,\"\",\"\",90\n",
      "101,Neena,Kochhar,NKOCHHAR,515.123.4568,1989-09-21,AD_VP,17000.0,\"\",100,90\n",
      "102,Lex,De Haan,LDEHAAN,515.123.4569,1993-01-13,AD_VP,17000.0,\"\",100,90\n",
      "103,Alexander,Hunold,AHUNOLD,590.423.4567,1990-01-03,IT_PROG,9000.0,\"\",102,60\n",
      "104,Bruce,Ernst,BERNST,590.423.4568,1991-05-21,IT_PROG,6000.0,\"\",103,60\n",
      "105,David,Austin,DAUSTIN,590.423.4569,1997-06-25,IT_PROG,4800.0,\"\",103,60\n",
      "106,Valli,Pataballa,VPATABAL,590.423.4560,1998-02-05,IT_PROG,4800.0,\"\",103,60\n",
      "107,Diana,Lorentz,DLORENTZ,590.423.5567,1999-02-07,IT_PROG,4200.0,\"\",103,60\n",
      "108,Nancy,Greenberg,NGREENBE,515.124.4569,1994-08-17,FI_MGR,12000.0,\"\",101,100\n",
      "109,Daniel,Faviet,DFAVIET,515.124.4169,1994-08-16,FI_ACCOUNT,9000.0,\"\",108,100\n",
      "110,John,Chen,JCHEN,515.124.4269,1997-09-28,FI_ACCOUNT,8200.0,\"\",108,100\n",
      "111,Ismael,Sciarra,ISCIARRA,515.124.4369,1997-09-30,FI_ACCOUNT,7700.0,\"\",108,100\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -head spark_write/non-part/default/part-00001-3c9f5af5-def1-4997-b1e6-fbac2b33c3db-c000.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f998ca4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path hdfs://m01.itversity.com:9000/user/itv005077/spark_write/non-part/default already exists.;",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-43d437f8ead3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'header'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'/user/{user}/spark_write/non-part/default/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    825\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path hdfs://m01.itversity.com:9000/user/itv005077/spark_write/non-part/default already exists.;"
     ]
    }
   ],
   "source": [
    "# deafult write mode is `errorIfExists` \n",
    "# AnalysisException: path <PATH> already exists.;\n",
    "df_emp.write \\\n",
    ".format('csv') \\\n",
    ".option('header', True) \\\n",
    ".save(f'/user/{user}/spark_write/non-part/default/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6b9e62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emp.write \\\n",
    ".format('csv') \\\n",
    ".option('header', True) \\\n",
    ".mode('append') \\\n",
    ".save(f'/user/{user}/spark_write/non-part/default/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "646aa96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\n",
      "-rw-r--r--   3 itv005077 supergroup          0 2023-07-02 15:50 spark_write/non-part/default/_SUCCESS\n",
      "-rw-r--r--   3 itv005077 supergroup      4.2 K 2023-07-02 15:49 spark_write/non-part/default/part-00000-3c9f5af5-def1-4997-b1e6-fbac2b33c3db-c000.csv\n",
      "-rw-r--r--   3 itv005077 supergroup      4.2 K 2023-07-02 15:50 spark_write/non-part/default/part-00000-f226e1ae-146f-451c-ba01-111a17cf6548-c000.csv\n",
      "-rw-r--r--   3 itv005077 supergroup      4.1 K 2023-07-02 15:49 spark_write/non-part/default/part-00001-3c9f5af5-def1-4997-b1e6-fbac2b33c3db-c000.csv\n",
      "-rw-r--r--   3 itv005077 supergroup      4.1 K 2023-07-02 15:50 spark_write/non-part/default/part-00001-f226e1ae-146f-451c-ba01-111a17cf6548-c000.csv\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls -h spark_write/non-part/default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a338f0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-02 15:50:44,370 INFO fs.TrashPolicyDefault: Moved: 'hdfs://m01.itversity.com:9000/user/itv005077/spark_write' to trash at: hdfs://m01.itversity.com:9000/user/itv005077/.Trash/Current/user/itv005077/spark_write1688327444343\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm -r spark_write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "820f6682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "drwx------   - itv005077 supergroup          0 2023-05-30 14:03 .Trash\n",
      "drwxr-xr-x   - itv005077 supergroup          0 2023-07-02 15:48 .sparkStaging\n",
      "drwxr-xr-x   - itv005077 supergroup          0 2023-06-25 15:30 warehouse\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09f75c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
